---
title: 网易公开课爬虫实践
tags: python 爬虫 英语学习 
key: 20190122
---
## 结果
这次先说结果吧,截止到目前(2019-01-28)位置总共爬了网易公开课4296个订阅号(有的订阅号是没有内容的),409030条内容(视频或者文章),243413个视频集合,内容里面的视频和视频集合里的视频是有重叠的.分别保存到*open163_subscribe,open163_content,open163*,3个collection的结构如下

<img width="800" src="https://user-images.githubusercontent.com/9245002/51821939-b0dbf000-2315-11e9-82dd-d09ae240cf7e.png">

<img width="800" src="https://user-images.githubusercontent.com/9245002/51821942-b1748680-2315-11e9-9c20-138716f838d2.png">

<img width="800" alt="20190128144850" src="https://user-images.githubusercontent.com/9245002/51821940-b0dbf000-2315-11e9-8f1b-35bff5cbf355.png">

## 数据抓取过程
* 首先,网易公开课的内容都是通过订阅号发布的,所以我先抓取了所有订阅号的信息
<img width="500" src="https://user-images.githubusercontent.com/9245002/51828496-a9711280-2326-11e9-8b94-17fc151d0d3b.jpeg">


通过在浏览器里调试订阅号的首页,可以发现获取订阅号的接口:https://c.open.163.com/open/mob/subscribe/detail/info.do?subscribeId=  
集合open163_subscribe保存的就是这个接口返回的信息.而且还发现订阅号的唯一标识subscribeId是数字的形式,所以只要从1开始自增就可以遍历所有的订阅号了(当然有的subscribeId是没有订阅号的,有可能是下架了,跳过就好)
```python
def crawler_all_subscribe():
    empty = 0
    subscribeId = 4080
    while empty < 100:
        subscribe = crawler_subscribe(subscribeId)
        if 'subscribeName' in subscribe.keys():
            print(subscribe['subscribeName'])
            insert_subscribe(subscribe)
            crawler_content(subscribeId)
            empty = 0
        else:
            empty = empty + 1
        subscribeId = subscribeId + 1
```

<img width="1369" alt="20190128203135" src="https://user-images.githubusercontent.com/9245002/51828561-cc032b80-2326-11e9-9a1a-aec8855228e0.png">

* 同时在订阅号首页还有一个列出推送内容的接口:
  https://c.open.163.com/open/mob/subscribe/detail/list.do?subscribeId=3994&rtypes=2%2C3%2C4%2C5%2C6%2C8%2C9%2C10%2C11%2C12&cursor=&pagesize=10  
  这里使用游标cursor来分页,最新一页cursor为空,每次请求结果会返回下一页的cursor,如不返回则为最后一页.接口返回的data字段(数组)即为存入open163_content中的内容.
  <img width="1200" alt="20190128203135" src="https://user-images.githubusercontent.com/9245002/51836606-cb28c480-233b-11e9-998b-dc5fd7523389.png">
  其中,如果内容是视频类型的话,还会有一个plid字段,这个plid即为视频集合的唯一标识,通过抓取所有订阅号的所有内容,就可以获得所有视频集合的plid.

```python
def crawler_content(subscribeId):
    content = []
    cursor = ''
    result = crawler_content_page(subscribeId, cursor)
    while 'data' in result.keys() and 'cursor' in result.keys():
        content = content + result['data']
        cursor = result['cursor']
        result = crawler_content_page(subscribeId, cursor)
    if 'data' in result.keys():
        content = content + result['data']
    print(len(content))
    insert_contents(content)


def crawler_content_page(subscribeId, cursor=''):
    # print(subscribe_url_prefix)
    url = content_url_prefix + '&subscribeId=' + str(
        subscribeId) + '&cursor=' + cursor
    response = urllib.request.urlopen(url)
    result = response.read().decode('utf-8')
    content = json.loads(result)
    if content is None or 'data' not in content.keys():
        return {}
    return content
```
* 有了视频集合的plid,就可以通过这个接口来获取视频信息了:https://c.open.163.com/mob/${plid}/getMoviesForAndroid.do
<img width="1353" alt="20190128210823" src="https://user-images.githubusercontent.com/9245002/51838315-e34f1280-2340-11e9-9966-8f84fd1ed186.png">
返回结果中的data即为存入*open163*中的内容.
```python
def insert_movies(plid):
    try:
        response = urllib.request.urlopen(movies_url_prefix + plid + movies_url_suffix)
        result = response.read().decode('utf-8')
        data = json.loads(result)
        if data['code'] == 200:
            try:
                collection.insert_one(data['data'])
            except pymongo.errors.DuplicateKeyError:
                pass
    except:
        print(plid)
```

## 分析
收集了这么多数据,可以用来做一些分析了,比如,可以看一下那些订阅号最受欢迎:

<img width="800" src="https://user-images.githubusercontent.com/9245002/51838939-b26fdd00-2342-11e9-8682-4063c841b3d9.png">

可以看出
![20190128212244](https://user-images.githubusercontent.com/9245002/51839004-e5b26c00-2342-11e9-819e-e6132817194b.png)
