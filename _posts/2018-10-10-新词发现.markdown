---
layout: post
title:  "新词发现"
# date:   2018-09-29 18:24:14 +0800
categories: NLP
---
## 参考资料
主要参考了以下两篇文章：  
[互联网时代的社会语言学：基于SNS的文本数据挖掘
](http://www.matrix67.com/blog/archives/5044) [基于信息熵和互信息的新词识别](http://www.hankcs.com/nlp/new-word-discovery.html)

## 分词依据
对于一个给定的文本，从中抽取一个片段，如果这个片段的内部成分搭配稳定，并且左右搭配很丰富，则认为是一个词。将这样的片段抽取出来，按照出现的频率排序，选择排在前面的那些作为我们发现的词语。再进一步通过固有词典过滤掉已经存在的“旧词”，剩下的就是“新词”了。  
如何理解“内部成分搭配稳定”和“左右搭配丰富”呢？  
假设有2个字符组成的片段“AB”出现在文本中若干次，如果A出现了，B总会紧接着出现，B出现了，A也出现，即A、B总是成对出现，而不会出现AC、AD或者EB、BF这种，我们就认为“AB”这个片段的内部成分搭配是最稳定的，A或B单独出现的次数越多，则“AB”的稳定性越低。  
即使A、B总是一起出现，但是假设“AB”后面跟的字符总是C，即“AB”的右搭配只有一个，我们也认为“AB”不能成词（可能“ABC”是一个词也说不定），如果“AB”后面可以接CDEFG……各种字，我们就说“AB”的右搭配很丰富，左搭配同理。  
内部搭配的稳定性和左右搭配的丰富程度涉及到两个概念，“*互信息*”和“*信息熵*”

### 互信息的概念
以下摘自维基百科
> 在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。

一般地，两个离散随机变量 X 和 Y 的互信息可以定义为：  

> $$I(X;Y) = \sum_{y \in Y}\sum_{x \in X}p(x,y)\log(\cfrac{p(x,y)}{p(x)p(y)})$$

由于我们在分词的时候是不考虑同义词的情况的，即不同的字符串代表不同的词。对于一个已知的文本片段和它的分割点，这两个“随机变量”只有一个值，上面的公式可以简化为：  
> $$I(x;y) = \log(\cfrac{p(x,y)}{p(x)p(y)})$$

对于一个长度大于2的文本片段，它的分割点不止一个，例如“ABC”可以分为“AB”和“C”或者“A”和“BC”,这时候一个文本片段就会有多个互信息值，我们取最小的一个作为词的聚合度。  
为什么要取最小的呢？考虑这样一种实际情况，“的蝙蝠”这个文本片段， 如果把它分割成“的蝙”和“蝠”这两个子片段，它的聚合度是很高的，“的蝙”几乎不会单独出现，“蝠”也很少和其他字搭配,但是"的蝙蝠"却不是一个词。因为把它拆成“的”和“蝙蝠”后，聚合度是很低的，因为“的”有无数种搭配，“蝙蝠”也可以组合成“只蝙蝠”、“蝙蝠侠”等。  
实际上，通过这种方法发现的新词以两字词居多，三字词较少，而且多是由两字词再加一个字组成的。这可能和中国人的用词习惯有关。聚合度很高的三字词（“加拿大”、“红领巾”）不多见，但是，四字词反而多了一些（成语）。  
而且，由两字词加一个字组成的词可能和那个两字词都作为新词被抽出来。例如前面提到的“蝙蝠侠”这个词，如果训练文本是关于影视的，“蝙蝠侠”很可能作为新词出现，而“蝙蝠”可能因为单独出现次数过少，就没有被筛选出来。这个时候如果再给训练集加入等量的关于动物的文本，“蝙蝠”就很有可能作为新词出现了，这个时候“蝙蝠侠”的词频和聚合度会降低，但是依然超过其他的词，所以它也是一个新词。
### 信息熵的概念
以下摘自维基百科：
> 在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。

[matrix67的文章](http://www.matrix67.com/blog/archives/5044)对这个概念有一个直观的解释，这里直接抄过来：
![20180928182345.png](http://upload-images.jianshu.io/upload_images/3936903-a24d9358dc30fa14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

依据Boltzmann's H-theorem，香农把随机变量X的熵值 Η（希腊字母Eta）定义如下，其值域为{x1, ..., xn}：
> $$H(X)=E[(I(X)]=E[-\ln(P(X))]$$  
 
其中，P为X的概率质量函数（probability mass function），E为期望函数，而I(X)是X的资讯量（又称为资讯本体）。I(X)本身是个随机变数。  
当取自有限的样本时，熵的公式可以表示為：
> $$H(X)=\sum_iP(x_i)I(x_i)=-\sum_iP(x_i)\log_bP(x_i)$$

对于一个词\\(w\\) ，我们统计出它所有的左邻字集合\\(left={l_1,l_2,l_3……l_n}\\)，每个字在文本中出现的概率为\\(p(l_i)\\),可以计算出\\(w\\)的左信息熵\\(leftEntropy=-p(l_1)\log p(l_1)-p(l_2)\log p(l_2)-p(l_3)\log p(l_3)-……-p(l_n)\log p(l_n)\\)，同理可求出右信息熵\\(rightEntropy\\)，取\\(leftEntropy\\)和\\(rightEntropy\\)的较小值作为最终信息熵。