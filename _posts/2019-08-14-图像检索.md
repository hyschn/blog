---
title: 图像检索
tags: c++ 图像处理 OpenCV 
key: 20190814
published: false
---
## 概述

图像检索按描述图像内容方式的不同可以分为两类，一类是基于文本的图像检索(TBIR, Text Based Image Retrieval)，另一类是基于内容的图像检索(CBIR, Content Based Image Retrieval)。

基于文本的图像检索方法始于上世纪70年代，它利用文本标注的方式对图像中的内容进行描述，从而为每幅图像形成描述这幅图像内容的关键词，比如图像中的物体、场景等，这种方式可以是人工标注方式，也可以通过图像识别技术进行半自动标注。在进行检索时，用户可以根据自己的兴趣提供查询关键字，检索系统根据用户提供的查询关键字找出那些标注有该查询关键字对应的图片，最后将查询的结果返回给用户。

![image](https://user-images.githubusercontent.com/9245002/63000629-32efc880-bea4-11e9-8f2c-32107b0dedac.png)

基于内容的图像检索基本框架如上图1.1所示，它利用计算机对图像进行分析，建立图像特征矢量描述并存入图像特征库，当用户输入一张查询图像时，用相同的特征提取方法提取查询图像的特征得到查询向量，然后在某种相似性度量准则下计算查询向量到特征库中各个特征的相似性大小，最后按相似性大小进行排序并顺序输出对应的图片。基于内容的图像检索技术将图像内容的表达和相似性度量交给计算机进行自动的处理，克服了采用文本进行图像检索所面临的缺陷，并且充分发挥了计算机长于计算的优势，大大提高了检索的效率，从而为海量图像库的检索开启了新的大门。

## 如何理解特征

你们大多数人都玩过拼图游戏。你会得到很多图像的小块，你需要把它们正确地组合起来形成一个大的真实图像。问题是，你是怎么做到的?把同样的理论投射到电脑程序上，让电脑玩拼图游戏，怎么样?如果计算机可以玩拼图游戏，为什么我们不能给计算机提供许多真实的自然美景的图像，并告诉它把所有这些图像拼接成一个大的单一图像呢?如果计算机可以把几个自然图像拼接到一个图像上，那么给一个建筑物或任何结构的大量图片，并让计算机从中创建一个三维模型，怎么样?

问题和想象还在继续。但这都取决于最基本的问题?你怎么玩拼图游戏?你如何将许多杂乱的图像块排列成一个大的单一图像?你怎么能把许多自然的图像缝合到一个单一的图像?

答案是，我们正在寻找特定的模式或特定的特性，它们是独特的，可以很容易地跟踪，可以很容易地进行比较。如果我们要定义这样一个特性，我们可能会发现很难用语言来表达它，但是我们知道它们是什么。如果有人让你指出一个可以在多个图像中进行比较的好特性，你可以指出一个。这就是为什么，即使是小孩子也可以简单地玩这些游戏。我们在一张图片中搜索这些特征，我们找到它们，我们在其他图片中找到相同的特征，我们对齐它们。就是这样。(在拼图游戏中，我们更关注不同图像的连续性)。所有这些能力都是与生俱来的。

所以我们的一个基本问题扩展到更多的数字，但变得更具体。这些特性是什么?(答案也应该是计算机可以理解的。)

嗯，很难说人类是如何发现这些特征的。它已经在我们的大脑中编程了。但是如果我们深入研究一些图片，寻找不同的图案，我们会发现一些有趣的东西。例如，如下图所示:
![image](https://user-images.githubusercontent.com/9245002/63006540-566d4000-beb1-11e9-8331-e0d87cdb3a3a.png)

像很简单。在图像的顶部，给出了六个小图像块。你的问题是找到这些补丁在原始图像中的确切位置。你能找到多少个正确的结果?

A和B是平面，它们分布在很多区域。很难找到这些补丁的确切位置。

C和D要简单得多。它们是建筑物的边缘。你可以找到一个大致的位置，但准确的位置仍然是困难的。这是因为，沿着边界，到处都是一样的。正常到边缘，它是不同的。因此，与平面区域相比，边缘是一个更好的特征，但还不够好(在拼图游戏中，边缘的连续性比较好)。

最后，E和F是建筑的一些角落。它们很容易被发现。因为在角落，无论你移动这个补丁，它看起来都不一样。所以它们可以被认为是一个很好的特性。因此，现在我们转向更简单(和广泛使用的图像)以更好地理解。

![image](https://user-images.githubusercontent.com/9245002/63006603-76046880-beb1-11e9-99f1-cd1e9184e1c9.png)

就像上面一样，蓝色的斑块是平坦的区域，很难找到和跟踪。无论你把蓝色的补丁移到哪里，它看起来都是一样的。对于black patch，它是一条边。如果你在垂直方向(即沿着梯度)移动它，它就会改变。沿着这条边(平行于这条边)放，它看起来是一样的。对于red patch，它是一个角。无论你移动补丁到哪里，它看起来都不一样，这意味着它是独一无二的。基本上，角被认为是图像中很好的特征。(不只是角，在某些情况下，blob被认为是很好的特性)。

现在我们回答了这个问题，“这些特征是什么?”但下一个问题出现了。我们怎么找到他们?或者我们怎么找到角?我们也用直观的方式回答了这个问题。，在图像中寻找那些在移动时(少量移动)周围所有区域变化最大的区域。这将在接下来的章节中被投射到计算机语言中。所以找到这些图像特征就叫做特征检测。

所以我们找到了图像中的特征(假设你做到了)。一旦你找到它，你应该会在其他图片中找到相同的。我们做什么?我们取一个特征周围的区域，用我们自己的话来解释，比如“上方是蓝天，下方是建筑区域，建筑上有一些玻璃等等”，然后在其他图像中搜索相同的区域。基本上，您是在描述特性。类似地，计算机也应该描述特征周围的区域，以便在其他图像中找到它。所谓的描述就是所谓的特征描述。一旦你有了特征和它的描述，你就可以在所有的图像中找到相同的特征，并对它们进行对齐、缝合或做任何你想做的事情。

## SIFT特征

关于SIFT网上有很多介绍,文后的参考资料也有详细的讲解.一个SIFT特征由两部分组成:关键点（keypoint)和对应特征描述子(Descriptor)。

一个SIFT keypoint是一块圆形区域并且带有方向，使用4个参数描述该区域的几何结构：

* keypoint的中心位置的坐标(x,y)
* keypoint的scale(圆形区域的半径r)
* keypoint的方向(使用弧度表示的角度θ)
![image](https://user-images.githubusercontent.com/9245002/63007329-03948800-beb3-11e9-91cf-7d9ebd86d049.png)

一个SIFT关键点由4个参数确定:
$$k(x,y,r,θ)$$
SIFT在多尺度空间检测关键点，确定关键点的位置和尺度，保证了关键点的尺度不变性；利用关键点邻域像素的梯度分布来确定关键点的方向，保证关键点的旋转（方向)不变性。

### 高斯尺度空间

SIFT在高斯尺度空间进行特征点检测，一个高斯尺度空间有图像和不同的高斯卷积核卷积得到：
$$L(x,y,σ)=G(x,y,σ)∗I(x,y)$$

L(x,y,σ)表示图像的高斯尺度空间，σ称为尺度空间因子，它是高斯函数标准差，反映了图像的模糊程度，其值越大图像越模糊，对应的尺度也就越大。

而对图像关键点的检测比较好的算子是Δ2G，高斯拉普拉斯(LoG)。但是该算子的运算量较大，所以通常使用DoG(Difference of Gaussian)来近似计算LoG。
DoG的定义为：
$$D(x,y,σ)=[G(x,y,kσ)−G(x,y,σ)]∗I(x,y)=L(x,y,kσ)−L(x,y,σ)$$

L(x,y,σ)表示高斯尺度空间，则相邻两个高斯尺度空间相减就得到来的DoG的响应图像。

所以为了的得到DoG的响应图像，就需要先构建高斯尺度空间。高斯尺度空间是由图像金字塔降采样结合高斯滤波得到的。高斯尺度空间分为多个组Octave（每组图像的分辨率一样），每组有多层Level（使用不同的σ进行高斯模糊得到）。以一个512×512的图像I，其构建高斯尺度空间的步骤：（倒立的金字塔）

* 高斯尺度的组数o=log2min(m,n)−3=log2(512)−3=6
* 构建第0组，将原图像进行上采样，宽和高增加一倍得到图像I0。
* 第0层I0∗G(x,y,σ0)
* 第1层I0∗G(x,y,kσ0)
* 第2层I0∗G(x,y,k2σ0)
* 构建第1组，将I0进行降采样，得到图像I1
* 第0层I1∗G(x,y,2σ0)
* 第1层I1∗G(x,y,2kσ0)
* 第2层I1∗G(x,y,2k2σ0)
* ...
* 构建第o组，第s层 Io∗G(x,y,2oksσ)
在Lowe的算法实现中σ0=1.6,omin=−1。omin=−1表示金字塔的第0组是原图像上采样得到的，宽和高加一倍

### DoG 极值点检测

高斯图像金字塔构建完成后，将同一组的相邻两层相减就得到了DoG金字塔。

每组的层数S=3，也就是说每组可以得到两层的DoG图像，以第一组为例：其尺度为σ,kσ，只有两项是无法求取极值的，需要左右两边都有尺度。由于无法比较取得极值，那么我们就需要继续对每组的图像进行高斯模糊，使得尺度形成$σ,kσ,k^2σ,k^3σ,k^4σ$这样就可以选择中间的三项$kσ,k^2σ,k^3σ$

检测关键点，就是在DoG的图像空间中寻找极值点，每个像素点要和其图像域（同一尺度空间）和尺度域（相邻的尺度空间）的所有相邻点进行比较，当其大于（或者小于）所有相邻点时，改点就是极值点。如图所示，中间的检测点要和其所在图像的3×3邻域8个像素点，以及其相邻的上下两层的3×3领域18个像素点，共26个像素点进行比较。
![image](https://user-images.githubusercontent.com/9245002/63008019-528eed00-beb4-11e9-861b-ca6f9d2bb5cd.png)

### 删除不好的极值点

删除两类极值点

* 在对比度比较低低的区域检测到的极值点
* 在图像的边缘部分检测到的极值点

### 确定关键点的方向

统计关键点邻域像素的梯度方向分布来确定关键点的方向。具体步骤如下：

* 计算以特征点为中心，以3×1.5σ为半径的区域图像的幅角和幅值，每个像点L(x,y)的梯度的模m(x,y)以及方向θ(x,y)可通过下面公式求得

$$m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L(x,y-1)]^2} \\
\theta(x,y)=\arctan\cfrac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}
$$

* 统计像素点的幅角和幅值的直方图，梯度方向的直方图的横轴是梯度方向的角度（梯度方向的范围是0到360度，直方图每36度一个柱共10个柱，或者没45度一个柱共8个柱），纵轴是梯度方向对应梯度幅值的累加，在直方图的峰值就是特征点的主方向。在梯度直方图中，当存在一个相当于主峰值80%能量的柱值时，则可以将这个方向认为是该特征点辅助方向。所以，一个特征点可能检测到多个方向（也可以理解为，一个特征点可能产生多个坐标、尺度相同，但是方向不同的特征点）。

得到特征点的主方向后，对于每个特征点可以得到三个信息k(x,y,r,θ)，即位置、尺度和方向。由此可以确定一个SIFT特征区域，一个SIFT特征区域由三个值表示，中心表示特征点位置，半径表示关键点的尺度，箭头表示主方向。
具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，一个特征点就产生了多个坐标、尺度相等，但是方向不同的特征点。

### 计算关键点描述子

在检测部分已经得到了SIFT关键点的位置，尺度和方向信息，生成关键点的描述子，就是使用一个向量来描述关键点及其邻域像素的信息。
由以下步骤生成描述子：

* 为了保证旋转不变性，将关键点为中心的邻域像素的坐标轴进行旋转，将x轴旋转至关键点主方向，如下图:
![image](https://user-images.githubusercontent.com/9245002/63008492-52432180-beb5-11e9-8ff0-2a6bb52f4365.png)
* 分块计算邻域内像素的梯度方向直方图，以关键点为中心的16×16的区域内，划分4×4个块，分别计算每个块的梯度直方图，如下图：
![image](https://user-images.githubusercontent.com/9245002/63008507-57a06c00-beb5-11e9-867f-ccada2688bcd.png)
每个块的梯度直方方向直方图的计算方式，和求关键点主方向时类似：此时每个区域的梯度直方图在0-360之间划分为8个方向区间，每个区间为45度，即每个种子点有8个方向的梯度强度信息，最后将得到的4×4×8=128维的特征向量。

* 为了去除光照变化的影响，需对上述生成的特征向量进行归一化处理。在归一化处理后，在128维的单位向量中，对大于0.2的要进行截断处理，即大于0.2的值只取0.2，然后重新进行一次归一化处理，其目的是为了提高鉴别性。0.2 是实验得出的经验值。

## 参考资料:

* [Understanding Features](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_meaning/py_features_meaning.html#features-meaning)
* [特征与匹配](http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB)
* [无痛理解SIFT](https://zhuanlan.zhihu.com/p/22476595)
* [SIFT特征提取分析](https://blog.csdn.net/abcjennifer/article/details/7639681)
* [SIFT算法详解](https://blog.csdn.net/zddblog/article/details/7521424) &nbsp; [zddhub](https://blog.csdn.net/zddblog)
* [SIFT特征详解](https://www.cnblogs.com/wangguchangqing/p/4853263.html) &nbsp; [Brook_icv](https://home.cnblogs.com/u/wangguchangqing/)
* [图像检索(1): 再论SIFT-基于vlfeat实现](https://www.cnblogs.com/wangguchangqing/p/9176103.html)
* [图像检索](https://www.zhihu.com/topic/19601593/hot)
* [图像检索：基于内容的图像检索技术](https://yongyuan.name/blog/cbir-technique-summary.html)
* [【图像检索】【TPAMI重磅综述】 SIFT与CNN的碰撞：万字长文回顾图像检索任务十年探索历程](https://cloud.tencent.com/developer/article/1333152)